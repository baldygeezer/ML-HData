{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Without the fancy charts</h1><br>\n",
    "<h2>Load the data</h2>\n",
    "The table shows the top 5 rows...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadData(csvLoc): \n",
    "    return pd.read_csv(csvLoc)\n",
    "dSet=loadData(\"housing.csv\")\n",
    "dSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and a quick overview for sanity purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\nlongitude             20640 non-null float64\nlatitude              20640 non-null float64\nhousing_median_age    20640 non-null float64\ntotal_rooms           20640 non-null float64\ntotal_bedrooms        20433 non-null float64\npopulation            20640 non-null float64\nhouseholds            20640 non-null float64\nmedian_income         20640 non-null float64\nmedian_house_value    20640 non-null float64\nocean_proximity       20640 non-null object\ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dSet.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2> Split the data </h1>\n",
    "We are making a stratified random split for this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as numpty\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def chopper (data, testPortion):\n",
    "    \n",
    "    #make a categoricl attribute out of median income\n",
    "    data[\"income_cat\"]= pd.cut(data[\"median_income\"],bins=[0., 1.5, 3.0, 4.5, 6., numpty.inf], labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    #make arrays of shuffled indices from the dataset that both have the same distribution\n",
    "    split=StratifiedShuffleSplit(n_splits=1,test_size=testPortion, random_state=42)\n",
    "    \n",
    "    #add the rows from each array to the respective output arrays\n",
    "    for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "        testSet=data.loc[test_index]\n",
    "        trainSet=data.loc[train_index]\n",
    "        \n",
    "     #remove the categoric attribute we created earlier   \n",
    "    for set_ in (testSet,trainSet):\n",
    "        set_.drop(\"income_cat\",axis=1,inplace=True)        \n",
    "    #victory!\n",
    "    return testSet, trainSet    \n",
    "            \n",
    "strat_test_set, strat_train_set = chopper(dSet,.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overview of the training set - looks right to me..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 17606 to 15775\nData columns (total 10 columns):\nlongitude             16512 non-null float64\nlatitude              16512 non-null float64\nhousing_median_age    16512 non-null float64\ntotal_rooms           16512 non-null float64\ntotal_bedrooms        16354 non-null float64\npopulation            16512 non-null float64\nhouseholds            16512 non-null float64\nmedian_income         16512 non-null float64\nmedian_house_value    16512 non-null float64\nocean_proximity       16512 non-null object\ndtypes: float64(9), object(1)\nmemory usage: 1.4+ MB\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "strat_train_set.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(16512, 16)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import  Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#drop the median house value and copy it into a set of labels before we start cleaning data\n",
    "housing=strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels=strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "#make a Transformer class to add some more attributes\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "#rooms_ix,bedrooms_ix, population_ix, household_ix=3,4,5,6\n",
    "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        #claculate the extra attributes\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        #claculate the optional one if needed\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "import bookeyWookey as book        \n",
    "\n",
    "#set up a pipeline to process the numeric attributes using the new class and others    \n",
    "num_pipe = Pipeline([('imputer',SimpleImputer(strategy=\"median\")),\n",
    "                    ('attribs_adder',book.CombinedAttributesAdder()),\n",
    "                    ('std_scaler',StandardScaler()),                    \n",
    "                    ])\n",
    "\n",
    "#split off the numeric attributes...\n",
    "housingNumeric = housing.drop(\"ocean_proximity\", axis=1)\n",
    "#...Store the column headings\n",
    "numeric=list(housingNumeric)\n",
    "#...same for the categorical attributes (1 of them)\n",
    "categoric=[\"ocean_proximity\"]\n",
    "\n",
    "\n",
    "#set up the pipeline to output the complete transformation\n",
    "full_pipe = ColumnTransformer([\n",
    "    (\"num\",num_pipe,numeric),\n",
    "    (\"cat\",OneHotEncoder(),categoric),\n",
    "    ])\n",
    "#execute the pipeline to produce prepared training data\n",
    "trainingDataPrepped=full_pipe.fit_transform(housing)\n",
    "trainingDataPrepped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "type(housing_labels)\n",
    "type(trainingDataPrepped)\n",
    "trainingDataPrepped \n",
    "linReg=LinearRegression()\n",
    "linReg.fit(trainingDataPrepped,housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK we has dun a AI!<br>\n",
    "now how accurate is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "predictions:  [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n 189747.55849879]\nLables:  [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "4709829587.971121"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "exampleData = housing.iloc[:5]\n",
    "exampleLabels = housing_labels.iloc[:5]\n",
    "preppedExamples=full_pipe.transform(exampleData)\n",
    "print(\"predictions: \",linReg.predict(preppedExamples))\n",
    "print(\"Lables: \",list(exampleLabels))\n",
    "\n",
    "\n",
    "\n",
    "predictions = linReg.predict(trainingDataPrepped)\n",
    "linRegMSE = mean_squared_error(housing_labels,predictions)\n",
    "linRegRMSE=np.sqrt(linRegMSE)\n",
    "linRegMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not brilliant, but better than the book!? <br>\n",
    "Lets try a decision tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "dtReg=DecisionTreeRegressor()\n",
    "dtReg.fit(trainingDataPrepped,housing_labels)\n",
    "\n",
    "\n",
    "dtPredictions=dtReg.predict(trainingDataPrepped)\n",
    "dtMSE=mean_squared_error(housing_labels,dtPredictions)\n",
    "dtRMSE=np.sqrt(dtMSE)\n",
    "dtRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bollocks! There is no way it is perfect! There is a whiff of over fitting here<br/>\n",
    "Let's do some cross-validation, i.e. splitting the training set up into smaller training and test sets. \n",
    "We are using k-fold validation which splits the traing data into 10 subsets, using each one in turn for validation and training on the other 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Scores: [68347.56445098 66908.28494553 71286.78319679 70196.89598448\n 71104.82820956 75841.10965079 71297.04508401 71225.80468213\n 77644.92515436 68428.3239582 ]\nMean: 71228.15653168518\nStandard deviation: 3135.910014218122\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#gets an array of scores mean squared errors\n",
    "scores = cross_val_score(dtReg, trainingDataPrepped, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "#Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), \n",
    "# so the scoring function is actually the opposite of the MSE (i.e., a negative value), we compute -scores before calculating the square root.\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def getYerScoresOut(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "getYerScoresOut(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ok lets do a random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/bernardroper/.conda/envs/ML-labs/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "22323.380310095537"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "r_ForestRegressor=RandomForestRegressor()\n",
    "r_ForestRegressor.fit(trainingDataPrepped,housing_labels)\n",
    "rf_Predictions=r_ForestRegressor.predict(trainingDataPrepped)\n",
    "\n",
    "rf_MSE=mean_squared_error(housing_labels,rf_Predictions)\n",
    "rf_RMSE=np.sqrt(rf_MSE)\n",
    "rf_RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So this is different from the book again...<br>\n",
    "...but it is the same as the result from his project on GitHub, running on the same data\n",
    "so I think we're good!<br>\n",
    "Now some finishing touches. This is a Grid Search, in which the random forest is trained with a range of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(d_set,labels ,nEstimators=[10,20,30], maxFeatures=[ 4, 6, 8]):\n",
    "    param_grid = [\n",
    "    {'n_estimators': nEstimators, 'max_features': maxFeatures},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},]\n",
    "    forest_reg = RandomForestRegressor()\n",
    "    gridSearch = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\n",
    "    gridSearch.fit(d_set,labels)\n",
    "    return gridSearch\n",
    "    \n",
    "gridSearch=grid_search(trainingDataPrepped, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features=6, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=30,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.3527318789663025, 'median_income'),\n (0.1512008881097525, 'INLAND'),\n (0.10480044914037438, 'pop_per_hhold'),\n (0.0771736040029538, 'longitude'),\n (0.06876382939912976, 'bedrooms_per_room'),\n (0.06560081638596547, 'latitude'),\n (0.04964804980135733, 'rooms_per_hhold'),\n (0.04142972584135312, 'housing_median_age'),\n (0.017677673770851117, 'population'),\n (0.017622264330721317, 'total_rooms'),\n (0.016445346996838882, 'households'),\n (0.015370090929360656, 'total_bedrooms'),\n (0.009247666500152055, '<1H OCEAN'),\n (0.0063197012472474045, 'NEAR OCEAN'),\n (0.0058416870336896465, 'NEAR BAY'),\n (0.00012632754395020692, 'ISLAND')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "extraAttribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "\n",
    "OneHotEncoder = full_pipe.named_transformers_[\"cat\"]\n",
    "cat1Hattribs = list(OneHotEncoder.categories_[0])\n",
    "attributes = numeric + extraAttribs + cat1Hattribs\n",
    "featureImportances = gridSearch.best_estimator_.feature_importances_\n",
    "sorted(zip(featureImportances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0      -1.156043  0.771950            0.743331    -0.493234       -0.445438   \n1      -1.176025  0.659695           -1.165317    -0.908967       -1.036928   \n2       1.186849 -1.342183            0.186642    -0.313660       -0.153345   \n3      -0.017068  0.313576           -0.290520    -0.362762       -0.396756   \n4       0.492474 -0.659299           -0.926736     1.856193        2.412211   \n5      -0.696456  0.945009           -0.370047     0.143693        0.131447   \n6       0.537433 -0.748168            1.856709    -0.182253       -0.528198   \n7       1.166867 -0.696718           -0.210993    -0.272040       -0.586617   \n8       0.657325 -0.771554            1.459074    -0.354345       -0.167949   \n9       0.647334 -0.757522            0.027588    -0.985660       -0.742400   \n10     -0.866304  1.113391           -0.370047     0.717488        1.015030   \n11     -1.021164  1.637247            0.981912    -0.205635        0.060857   \n12     -1.555683  1.342578           -0.847209     0.346649       -0.080321   \n13      1.256786 -0.687363           -0.608628    -0.048040        0.116842   \n14      0.057865  0.594213           -1.324371     0.167075       -0.094926   \n15      0.807191 -0.846391            0.107115     0.100670        0.279928   \n16     -1.415809  1.015168           -0.290520    -0.675614        0.026780   \n17      0.277667 -0.135444            1.061439    -0.647088       -0.688850   \n18      0.927083 -0.743490           -1.642479    -0.365568       -0.304260   \n19      0.712276 -0.673331            1.379547    -0.522695       -0.664508   \n20      0.637343 -0.678008           -1.165317     0.464027        0.922534   \n21     -1.235971  0.781304           -0.290520     0.695976        0.009741   \n22     -0.541596 -0.069962           -0.926736     0.841413        1.000425   \n23      1.161872 -0.692040            0.663804    -0.427297       -0.623128   \n24      1.191845 -0.327212            0.186642    -0.275313       -0.219066   \n25      1.256786 -1.225251           -0.370047    -0.838353       -0.837330   \n26      0.752240 -0.715427            0.981912     0.115167        0.070594   \n27      0.667317 -0.785586            1.220493    -0.600791       -0.408927   \n28      0.647334 -0.808972            0.981912    -0.936557       -0.976075   \n29     -1.440787  1.005814            1.856709    -0.263622       -0.287221   \n...          ...       ...                 ...          ...             ...   \n16482   1.551521 -0.874454           -0.210993     0.784361        0.567153   \n16483   0.672312 -0.869777           -0.688155    -1.198904       -1.241393   \n16484  -0.741416  1.688697           -0.926736     0.235818        0.012175   \n16485  -1.225980  0.771950           -0.529101     0.231609       -0.233670   \n16486   0.702285 -0.724781            0.345696     0.132937        0.640176   \n16487  -1.640607  2.301422           -1.324371    -0.595180       -0.569578   \n16488  -1.450778  1.342578            1.618128    -0.301969       -0.343205   \n16489   1.201836 -1.337506           -0.926736     0.571584        1.258441   \n16490  -1.245962  0.968396            0.902385    -0.381000       -0.564710   \n16491   0.782213 -0.827682           -0.290520     1.742089        2.631281   \n16492  -1.315899  1.001137            1.140966    -0.430570       -0.180120   \n16493   1.171863 -0.794941           -1.642479    -0.101819        0.007307   \n16494  -0.856313  1.169519            0.425223     0.068870        0.024346   \n16495   0.877128 -0.879132            0.345696     0.070741       -0.258011   \n16496   0.117811  0.313576           -1.165317    -0.461902       -0.272616   \n16497  -1.280930  1.118069            1.061439    -1.171313       -1.217052   \n16498  -0.396726  1.211614           -1.244844     0.831592        0.718068   \n16499   0.577397 -0.748168            1.856709    -0.591438       -0.637733   \n16500   0.802195 -0.823004           -1.006263    -0.289810       -0.569578   \n16501   0.952060 -0.953968           -0.290520     0.253121       -0.206895   \n16502   1.141889 -1.197187           -0.847209     0.281647       -0.046244   \n16503   0.602375 -0.682686            0.584277    -0.474528       -0.391888   \n16504   1.216822 -0.855745           -1.244844     0.876018        0.540378   \n16505   0.902105 -0.729459           -0.131466     1.546147        0.968782   \n16506   0.687299 -0.780909            1.140966    -0.840691       -0.783780   \n16507   0.722267 -0.673331            1.379547    -0.632123       -0.725361   \n16508   1.007011 -0.823004            0.902385    -0.667196       -0.584183   \n16509   1.586489 -0.724781           -1.562952     1.043901        0.822735   \n16510   0.782213 -0.851068            0.186642    -0.309919       -0.374849   \n16511  -1.435791  0.996459            1.856709     0.220853        0.360253   \n\n       population  households  median_income  rooms_per_hhold  pop_per_hhold  \\\n0       -0.636211   -0.420698      -0.614937        -0.312055      -0.086499   \n1       -0.998331   -1.022227       1.336459         0.217683      -0.033534   \n2       -0.433639   -0.093318      -0.532046        -0.465315      -0.092405   \n3        0.036041   -0.383436      -1.045566        -0.079661       0.089736   \n4        2.724154    2.570975      -0.441437        -0.357834      -0.004194   \n5        0.025285    0.194138      -0.176435        -0.114867      -0.048003   \n6       -0.583328   -0.585720       2.366702         1.003599      -0.027646   \n7       -0.433639   -0.324880       1.115239         0.000891      -0.051831   \n8        0.444770   -0.157197      -1.076906        -0.452724       0.110319   \n9        1.138535   -0.745417      -1.772116        -1.174387       0.802788   \n10       1.177974    1.016582      -0.565643        -0.272283       0.001201   \n11      -0.195214    0.023794      -1.147251        -0.431191      -0.062234   \n12       0.019907    0.023794       1.475050         0.462500      -0.021290   \n13      -0.899734   -0.921085      -0.610843         4.307038      -0.029477   \n14      -0.211348   -0.181152       0.020217         0.576669      -0.029050   \n15       0.303149    0.255356      -0.258539        -0.250612      -0.011382   \n16      -0.741979   -0.149212      -0.106457        -1.060309      -0.151412   \n17      -0.577053   -0.599028      -0.993542        -0.338943      -0.021019   \n18      -0.158464   -0.274309       0.097544        -0.293974       0.005040   \n19      -0.504450   -0.607013       0.327793         0.059141       0.007720   \n20       0.451941    0.944718      -0.192814        -0.458481      -0.072357   \n21       0.148083    0.188815       2.810769         0.688204      -0.026410   \n22       0.795238    1.032552      -0.549684        -0.169905      -0.042268   \n23      -0.605736   -0.593704      -0.082309         0.305129      -0.032898   \n24      -0.289329   -0.346173      -0.904718         0.039015      -0.009266   \n25      -1.022532   -0.801312      -1.027874        -0.461645      -0.144415   \n26       0.185729    0.095658       0.628913        -0.022056      -0.003790   \n27       0.309424   -0.290278      -1.101264        -0.762698       0.125386   \n28      -0.697162   -0.891807      -0.640976        -0.617690       0.074801   \n29      -0.377170   -0.253016      -0.070182        -0.121935      -0.052774   \n...           ...         ...            ...              ...            ...   \n16482    0.122986    0.457640       0.280914         0.377987      -0.066389   \n16483   -1.210763   -1.261774      -0.691267        -1.100891      -0.008325   \n16484   -0.026703    0.061057       0.041005         0.219449      -0.036546   \n16485   -0.244512   -0.191798       3.406866         0.726032      -0.034323   \n16486    1.606422    0.787682      -0.861356        -0.679466       0.082352   \n16487   -0.893460   -0.865191      -1.121475         0.922219      -0.054999   \n16488   -0.608425   -0.420698      -0.390778         0.149913      -0.078605   \n16489    1.507825    1.189588      -0.792585        -0.523522       0.016365   \n16490   -0.603943   -0.604351       0.748552         0.480908      -0.028786   \n16491    2.481247    2.661470      -0.386683        -0.459429      -0.025797   \n16492   -0.172805   -0.239708      -1.132237        -0.481891      -0.007052   \n16493    0.156150    0.053072      -0.418233        -0.301920      -0.001145   \n16494   -0.009672    0.100981      -0.336916        -0.100615      -0.039949   \n16495   -0.171013   -0.239708       1.269789         0.526638      -0.006628   \n16496   -0.492798   -0.261001      -0.617142        -0.514083      -0.079070   \n16497   -1.224208   -1.243142      -0.680400        -0.577036      -0.111912   \n16498   -0.444395   -0.303587      -0.620187         2.316758      -0.059036   \n16499   -0.758113   -0.612336       0.929087        -0.135623      -0.081715   \n16500   -0.222104   -0.476593       1.135923         0.328684       0.050852   \n16501    0.087132   -0.117272       1.531064         0.591280       0.021783   \n16502    0.038730   -0.056055       0.999065         0.511125      -0.001977   \n16503   -0.181769   -0.327541      -0.473932        -0.436844       0.013602   \n16504    0.559501    0.526842      -0.313818         0.393901      -0.013417   \n16505    1.252370    0.880839       1.138023         0.658702       0.026392   \n16506   -0.537614   -0.833251      -1.042521        -0.366301       0.117406   \n16507   -0.759010   -0.764049       0.554158         0.234352      -0.031755   \n16508   -0.329664   -0.636291      -0.948815        -0.308114       0.084689   \n16509    0.607904    0.713156      -0.316705         0.346934      -0.030554   \n16510   -0.057178   -0.375451       0.098121         0.024995       0.061509   \n16511   -0.135159    0.377791      -0.157799        -0.228529      -0.095863   \n\n       bedrooms_per_room  INLAND  \n0               0.155318     0.0  \n1              -0.836289     0.0  \n2               0.422200     0.0  \n3              -0.196453     1.0  \n4               0.269928     0.0  \n5              -0.199264     1.0  \n6              -1.097923     0.0  \n7              -1.073504     1.0  \n8               0.545225     0.0  \n9               3.534531     0.0  \n10              0.230593     1.0  \n11              0.648370     1.0  \n12             -0.991280     0.0  \n13              0.263980     1.0  \n14             -0.728398     1.0  \n15              0.229237     0.0  \n16              3.809822     0.0  \n17             -0.170185     1.0  \n18              0.129451     1.0  \n19             -0.616457     1.0  \n20              0.594641     0.0  \n21             -1.267690     0.0  \n22              0.000017     0.0  \n23             -0.781030     1.0  \n24              0.070183     1.0  \n25              0.232810     0.0  \n26             -0.267320     0.0  \n27              0.915774     0.0  \n28              0.012442     0.0  \n29             -0.178493     0.0  \n...                  ...     ...  \n16482          -0.540701     1.0  \n16483           2.954813     0.0  \n16484          -0.632560     1.0  \n16485          -1.120677     0.0  \n16486           0.925312     0.0  \n16487           0.130375     1.0  \n16488          -0.228289     1.0  \n16489           0.912704     0.0  \n16490          -0.714196     0.0  \n16491           0.622985     0.0  \n16492           0.865695     0.0  \n16493           0.146637     1.0  \n16494          -0.264959     1.0  \n16495          -0.909252     0.0  \n16496           0.679504     1.0  \n16497           1.139103     0.0  \n16498          -0.387751     1.0  \n16499          -0.205203     0.0  \n16500          -0.978341     0.0  \n16501          -1.098726     0.0  \n16502          -0.826610     0.0  \n16503           0.279480     0.0  \n16504          -0.697146     1.0  \n16505          -0.864815     1.0  \n16506           0.662145     0.0  \n16507          -0.428853     1.0  \n16508           0.491503     1.0  \n16509          -0.521776     1.0  \n16510          -0.303407     0.0  \n16511           0.101806     0.0  \n\n[16512 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>rooms_per_hhold</th>\n      <th>pop_per_hhold</th>\n      <th>bedrooms_per_room</th>\n      <th>INLAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.156043</td>\n      <td>0.771950</td>\n      <td>0.743331</td>\n      <td>-0.493234</td>\n      <td>-0.445438</td>\n      <td>-0.636211</td>\n      <td>-0.420698</td>\n      <td>-0.614937</td>\n      <td>-0.312055</td>\n      <td>-0.086499</td>\n      <td>0.155318</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.176025</td>\n      <td>0.659695</td>\n      <td>-1.165317</td>\n      <td>-0.908967</td>\n      <td>-1.036928</td>\n      <td>-0.998331</td>\n      <td>-1.022227</td>\n      <td>1.336459</td>\n      <td>0.217683</td>\n      <td>-0.033534</td>\n      <td>-0.836289</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.186849</td>\n      <td>-1.342183</td>\n      <td>0.186642</td>\n      <td>-0.313660</td>\n      <td>-0.153345</td>\n      <td>-0.433639</td>\n      <td>-0.093318</td>\n      <td>-0.532046</td>\n      <td>-0.465315</td>\n      <td>-0.092405</td>\n      <td>0.422200</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.017068</td>\n      <td>0.313576</td>\n      <td>-0.290520</td>\n      <td>-0.362762</td>\n      <td>-0.396756</td>\n      <td>0.036041</td>\n      <td>-0.383436</td>\n      <td>-1.045566</td>\n      <td>-0.079661</td>\n      <td>0.089736</td>\n      <td>-0.196453</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.492474</td>\n      <td>-0.659299</td>\n      <td>-0.926736</td>\n      <td>1.856193</td>\n      <td>2.412211</td>\n      <td>2.724154</td>\n      <td>2.570975</td>\n      <td>-0.441437</td>\n      <td>-0.357834</td>\n      <td>-0.004194</td>\n      <td>0.269928</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.696456</td>\n      <td>0.945009</td>\n      <td>-0.370047</td>\n      <td>0.143693</td>\n      <td>0.131447</td>\n      <td>0.025285</td>\n      <td>0.194138</td>\n      <td>-0.176435</td>\n      <td>-0.114867</td>\n      <td>-0.048003</td>\n      <td>-0.199264</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.537433</td>\n      <td>-0.748168</td>\n      <td>1.856709</td>\n      <td>-0.182253</td>\n      <td>-0.528198</td>\n      <td>-0.583328</td>\n      <td>-0.585720</td>\n      <td>2.366702</td>\n      <td>1.003599</td>\n      <td>-0.027646</td>\n      <td>-1.097923</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.166867</td>\n      <td>-0.696718</td>\n      <td>-0.210993</td>\n      <td>-0.272040</td>\n      <td>-0.586617</td>\n      <td>-0.433639</td>\n      <td>-0.324880</td>\n      <td>1.115239</td>\n      <td>0.000891</td>\n      <td>-0.051831</td>\n      <td>-1.073504</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.657325</td>\n      <td>-0.771554</td>\n      <td>1.459074</td>\n      <td>-0.354345</td>\n      <td>-0.167949</td>\n      <td>0.444770</td>\n      <td>-0.157197</td>\n      <td>-1.076906</td>\n      <td>-0.452724</td>\n      <td>0.110319</td>\n      <td>0.545225</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.647334</td>\n      <td>-0.757522</td>\n      <td>0.027588</td>\n      <td>-0.985660</td>\n      <td>-0.742400</td>\n      <td>1.138535</td>\n      <td>-0.745417</td>\n      <td>-1.772116</td>\n      <td>-1.174387</td>\n      <td>0.802788</td>\n      <td>3.534531</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.866304</td>\n      <td>1.113391</td>\n      <td>-0.370047</td>\n      <td>0.717488</td>\n      <td>1.015030</td>\n      <td>1.177974</td>\n      <td>1.016582</td>\n      <td>-0.565643</td>\n      <td>-0.272283</td>\n      <td>0.001201</td>\n      <td>0.230593</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-1.021164</td>\n      <td>1.637247</td>\n      <td>0.981912</td>\n      <td>-0.205635</td>\n      <td>0.060857</td>\n      <td>-0.195214</td>\n      <td>0.023794</td>\n      <td>-1.147251</td>\n      <td>-0.431191</td>\n      <td>-0.062234</td>\n      <td>0.648370</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-1.555683</td>\n      <td>1.342578</td>\n      <td>-0.847209</td>\n      <td>0.346649</td>\n      <td>-0.080321</td>\n      <td>0.019907</td>\n      <td>0.023794</td>\n      <td>1.475050</td>\n      <td>0.462500</td>\n      <td>-0.021290</td>\n      <td>-0.991280</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.256786</td>\n      <td>-0.687363</td>\n      <td>-0.608628</td>\n      <td>-0.048040</td>\n      <td>0.116842</td>\n      <td>-0.899734</td>\n      <td>-0.921085</td>\n      <td>-0.610843</td>\n      <td>4.307038</td>\n      <td>-0.029477</td>\n      <td>0.263980</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.057865</td>\n      <td>0.594213</td>\n      <td>-1.324371</td>\n      <td>0.167075</td>\n      <td>-0.094926</td>\n      <td>-0.211348</td>\n      <td>-0.181152</td>\n      <td>0.020217</td>\n      <td>0.576669</td>\n      <td>-0.029050</td>\n      <td>-0.728398</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.807191</td>\n      <td>-0.846391</td>\n      <td>0.107115</td>\n      <td>0.100670</td>\n      <td>0.279928</td>\n      <td>0.303149</td>\n      <td>0.255356</td>\n      <td>-0.258539</td>\n      <td>-0.250612</td>\n      <td>-0.011382</td>\n      <td>0.229237</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-1.415809</td>\n      <td>1.015168</td>\n      <td>-0.290520</td>\n      <td>-0.675614</td>\n      <td>0.026780</td>\n      <td>-0.741979</td>\n      <td>-0.149212</td>\n      <td>-0.106457</td>\n      <td>-1.060309</td>\n      <td>-0.151412</td>\n      <td>3.809822</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.277667</td>\n      <td>-0.135444</td>\n      <td>1.061439</td>\n      <td>-0.647088</td>\n      <td>-0.688850</td>\n      <td>-0.577053</td>\n      <td>-0.599028</td>\n      <td>-0.993542</td>\n      <td>-0.338943</td>\n      <td>-0.021019</td>\n      <td>-0.170185</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.927083</td>\n      <td>-0.743490</td>\n      <td>-1.642479</td>\n      <td>-0.365568</td>\n      <td>-0.304260</td>\n      <td>-0.158464</td>\n      <td>-0.274309</td>\n      <td>0.097544</td>\n      <td>-0.293974</td>\n      <td>0.005040</td>\n      <td>0.129451</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.712276</td>\n      <td>-0.673331</td>\n      <td>1.379547</td>\n      <td>-0.522695</td>\n      <td>-0.664508</td>\n      <td>-0.504450</td>\n      <td>-0.607013</td>\n      <td>0.327793</td>\n      <td>0.059141</td>\n      <td>0.007720</td>\n      <td>-0.616457</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.637343</td>\n      <td>-0.678008</td>\n      <td>-1.165317</td>\n      <td>0.464027</td>\n      <td>0.922534</td>\n      <td>0.451941</td>\n      <td>0.944718</td>\n      <td>-0.192814</td>\n      <td>-0.458481</td>\n      <td>-0.072357</td>\n      <td>0.594641</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-1.235971</td>\n      <td>0.781304</td>\n      <td>-0.290520</td>\n      <td>0.695976</td>\n      <td>0.009741</td>\n      <td>0.148083</td>\n      <td>0.188815</td>\n      <td>2.810769</td>\n      <td>0.688204</td>\n      <td>-0.026410</td>\n      <td>-1.267690</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-0.541596</td>\n      <td>-0.069962</td>\n      <td>-0.926736</td>\n      <td>0.841413</td>\n      <td>1.000425</td>\n      <td>0.795238</td>\n      <td>1.032552</td>\n      <td>-0.549684</td>\n      <td>-0.169905</td>\n      <td>-0.042268</td>\n      <td>0.000017</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.161872</td>\n      <td>-0.692040</td>\n      <td>0.663804</td>\n      <td>-0.427297</td>\n      <td>-0.623128</td>\n      <td>-0.605736</td>\n      <td>-0.593704</td>\n      <td>-0.082309</td>\n      <td>0.305129</td>\n      <td>-0.032898</td>\n      <td>-0.781030</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.191845</td>\n      <td>-0.327212</td>\n      <td>0.186642</td>\n      <td>-0.275313</td>\n      <td>-0.219066</td>\n      <td>-0.289329</td>\n      <td>-0.346173</td>\n      <td>-0.904718</td>\n      <td>0.039015</td>\n      <td>-0.009266</td>\n      <td>0.070183</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.256786</td>\n      <td>-1.225251</td>\n      <td>-0.370047</td>\n      <td>-0.838353</td>\n      <td>-0.837330</td>\n      <td>-1.022532</td>\n      <td>-0.801312</td>\n      <td>-1.027874</td>\n      <td>-0.461645</td>\n      <td>-0.144415</td>\n      <td>0.232810</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.752240</td>\n      <td>-0.715427</td>\n      <td>0.981912</td>\n      <td>0.115167</td>\n      <td>0.070594</td>\n      <td>0.185729</td>\n      <td>0.095658</td>\n      <td>0.628913</td>\n      <td>-0.022056</td>\n      <td>-0.003790</td>\n      <td>-0.267320</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.667317</td>\n      <td>-0.785586</td>\n      <td>1.220493</td>\n      <td>-0.600791</td>\n      <td>-0.408927</td>\n      <td>0.309424</td>\n      <td>-0.290278</td>\n      <td>-1.101264</td>\n      <td>-0.762698</td>\n      <td>0.125386</td>\n      <td>0.915774</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.647334</td>\n      <td>-0.808972</td>\n      <td>0.981912</td>\n      <td>-0.936557</td>\n      <td>-0.976075</td>\n      <td>-0.697162</td>\n      <td>-0.891807</td>\n      <td>-0.640976</td>\n      <td>-0.617690</td>\n      <td>0.074801</td>\n      <td>0.012442</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-1.440787</td>\n      <td>1.005814</td>\n      <td>1.856709</td>\n      <td>-0.263622</td>\n      <td>-0.287221</td>\n      <td>-0.377170</td>\n      <td>-0.253016</td>\n      <td>-0.070182</td>\n      <td>-0.121935</td>\n      <td>-0.052774</td>\n      <td>-0.178493</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16482</th>\n      <td>1.551521</td>\n      <td>-0.874454</td>\n      <td>-0.210993</td>\n      <td>0.784361</td>\n      <td>0.567153</td>\n      <td>0.122986</td>\n      <td>0.457640</td>\n      <td>0.280914</td>\n      <td>0.377987</td>\n      <td>-0.066389</td>\n      <td>-0.540701</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16483</th>\n      <td>0.672312</td>\n      <td>-0.869777</td>\n      <td>-0.688155</td>\n      <td>-1.198904</td>\n      <td>-1.241393</td>\n      <td>-1.210763</td>\n      <td>-1.261774</td>\n      <td>-0.691267</td>\n      <td>-1.100891</td>\n      <td>-0.008325</td>\n      <td>2.954813</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16484</th>\n      <td>-0.741416</td>\n      <td>1.688697</td>\n      <td>-0.926736</td>\n      <td>0.235818</td>\n      <td>0.012175</td>\n      <td>-0.026703</td>\n      <td>0.061057</td>\n      <td>0.041005</td>\n      <td>0.219449</td>\n      <td>-0.036546</td>\n      <td>-0.632560</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16485</th>\n      <td>-1.225980</td>\n      <td>0.771950</td>\n      <td>-0.529101</td>\n      <td>0.231609</td>\n      <td>-0.233670</td>\n      <td>-0.244512</td>\n      <td>-0.191798</td>\n      <td>3.406866</td>\n      <td>0.726032</td>\n      <td>-0.034323</td>\n      <td>-1.120677</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16486</th>\n      <td>0.702285</td>\n      <td>-0.724781</td>\n      <td>0.345696</td>\n      <td>0.132937</td>\n      <td>0.640176</td>\n      <td>1.606422</td>\n      <td>0.787682</td>\n      <td>-0.861356</td>\n      <td>-0.679466</td>\n      <td>0.082352</td>\n      <td>0.925312</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16487</th>\n      <td>-1.640607</td>\n      <td>2.301422</td>\n      <td>-1.324371</td>\n      <td>-0.595180</td>\n      <td>-0.569578</td>\n      <td>-0.893460</td>\n      <td>-0.865191</td>\n      <td>-1.121475</td>\n      <td>0.922219</td>\n      <td>-0.054999</td>\n      <td>0.130375</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16488</th>\n      <td>-1.450778</td>\n      <td>1.342578</td>\n      <td>1.618128</td>\n      <td>-0.301969</td>\n      <td>-0.343205</td>\n      <td>-0.608425</td>\n      <td>-0.420698</td>\n      <td>-0.390778</td>\n      <td>0.149913</td>\n      <td>-0.078605</td>\n      <td>-0.228289</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16489</th>\n      <td>1.201836</td>\n      <td>-1.337506</td>\n      <td>-0.926736</td>\n      <td>0.571584</td>\n      <td>1.258441</td>\n      <td>1.507825</td>\n      <td>1.189588</td>\n      <td>-0.792585</td>\n      <td>-0.523522</td>\n      <td>0.016365</td>\n      <td>0.912704</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16490</th>\n      <td>-1.245962</td>\n      <td>0.968396</td>\n      <td>0.902385</td>\n      <td>-0.381000</td>\n      <td>-0.564710</td>\n      <td>-0.603943</td>\n      <td>-0.604351</td>\n      <td>0.748552</td>\n      <td>0.480908</td>\n      <td>-0.028786</td>\n      <td>-0.714196</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16491</th>\n      <td>0.782213</td>\n      <td>-0.827682</td>\n      <td>-0.290520</td>\n      <td>1.742089</td>\n      <td>2.631281</td>\n      <td>2.481247</td>\n      <td>2.661470</td>\n      <td>-0.386683</td>\n      <td>-0.459429</td>\n      <td>-0.025797</td>\n      <td>0.622985</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16492</th>\n      <td>-1.315899</td>\n      <td>1.001137</td>\n      <td>1.140966</td>\n      <td>-0.430570</td>\n      <td>-0.180120</td>\n      <td>-0.172805</td>\n      <td>-0.239708</td>\n      <td>-1.132237</td>\n      <td>-0.481891</td>\n      <td>-0.007052</td>\n      <td>0.865695</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16493</th>\n      <td>1.171863</td>\n      <td>-0.794941</td>\n      <td>-1.642479</td>\n      <td>-0.101819</td>\n      <td>0.007307</td>\n      <td>0.156150</td>\n      <td>0.053072</td>\n      <td>-0.418233</td>\n      <td>-0.301920</td>\n      <td>-0.001145</td>\n      <td>0.146637</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16494</th>\n      <td>-0.856313</td>\n      <td>1.169519</td>\n      <td>0.425223</td>\n      <td>0.068870</td>\n      <td>0.024346</td>\n      <td>-0.009672</td>\n      <td>0.100981</td>\n      <td>-0.336916</td>\n      <td>-0.100615</td>\n      <td>-0.039949</td>\n      <td>-0.264959</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16495</th>\n      <td>0.877128</td>\n      <td>-0.879132</td>\n      <td>0.345696</td>\n      <td>0.070741</td>\n      <td>-0.258011</td>\n      <td>-0.171013</td>\n      <td>-0.239708</td>\n      <td>1.269789</td>\n      <td>0.526638</td>\n      <td>-0.006628</td>\n      <td>-0.909252</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16496</th>\n      <td>0.117811</td>\n      <td>0.313576</td>\n      <td>-1.165317</td>\n      <td>-0.461902</td>\n      <td>-0.272616</td>\n      <td>-0.492798</td>\n      <td>-0.261001</td>\n      <td>-0.617142</td>\n      <td>-0.514083</td>\n      <td>-0.079070</td>\n      <td>0.679504</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16497</th>\n      <td>-1.280930</td>\n      <td>1.118069</td>\n      <td>1.061439</td>\n      <td>-1.171313</td>\n      <td>-1.217052</td>\n      <td>-1.224208</td>\n      <td>-1.243142</td>\n      <td>-0.680400</td>\n      <td>-0.577036</td>\n      <td>-0.111912</td>\n      <td>1.139103</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16498</th>\n      <td>-0.396726</td>\n      <td>1.211614</td>\n      <td>-1.244844</td>\n      <td>0.831592</td>\n      <td>0.718068</td>\n      <td>-0.444395</td>\n      <td>-0.303587</td>\n      <td>-0.620187</td>\n      <td>2.316758</td>\n      <td>-0.059036</td>\n      <td>-0.387751</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16499</th>\n      <td>0.577397</td>\n      <td>-0.748168</td>\n      <td>1.856709</td>\n      <td>-0.591438</td>\n      <td>-0.637733</td>\n      <td>-0.758113</td>\n      <td>-0.612336</td>\n      <td>0.929087</td>\n      <td>-0.135623</td>\n      <td>-0.081715</td>\n      <td>-0.205203</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16500</th>\n      <td>0.802195</td>\n      <td>-0.823004</td>\n      <td>-1.006263</td>\n      <td>-0.289810</td>\n      <td>-0.569578</td>\n      <td>-0.222104</td>\n      <td>-0.476593</td>\n      <td>1.135923</td>\n      <td>0.328684</td>\n      <td>0.050852</td>\n      <td>-0.978341</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16501</th>\n      <td>0.952060</td>\n      <td>-0.953968</td>\n      <td>-0.290520</td>\n      <td>0.253121</td>\n      <td>-0.206895</td>\n      <td>0.087132</td>\n      <td>-0.117272</td>\n      <td>1.531064</td>\n      <td>0.591280</td>\n      <td>0.021783</td>\n      <td>-1.098726</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16502</th>\n      <td>1.141889</td>\n      <td>-1.197187</td>\n      <td>-0.847209</td>\n      <td>0.281647</td>\n      <td>-0.046244</td>\n      <td>0.038730</td>\n      <td>-0.056055</td>\n      <td>0.999065</td>\n      <td>0.511125</td>\n      <td>-0.001977</td>\n      <td>-0.826610</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16503</th>\n      <td>0.602375</td>\n      <td>-0.682686</td>\n      <td>0.584277</td>\n      <td>-0.474528</td>\n      <td>-0.391888</td>\n      <td>-0.181769</td>\n      <td>-0.327541</td>\n      <td>-0.473932</td>\n      <td>-0.436844</td>\n      <td>0.013602</td>\n      <td>0.279480</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16504</th>\n      <td>1.216822</td>\n      <td>-0.855745</td>\n      <td>-1.244844</td>\n      <td>0.876018</td>\n      <td>0.540378</td>\n      <td>0.559501</td>\n      <td>0.526842</td>\n      <td>-0.313818</td>\n      <td>0.393901</td>\n      <td>-0.013417</td>\n      <td>-0.697146</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16505</th>\n      <td>0.902105</td>\n      <td>-0.729459</td>\n      <td>-0.131466</td>\n      <td>1.546147</td>\n      <td>0.968782</td>\n      <td>1.252370</td>\n      <td>0.880839</td>\n      <td>1.138023</td>\n      <td>0.658702</td>\n      <td>0.026392</td>\n      <td>-0.864815</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16506</th>\n      <td>0.687299</td>\n      <td>-0.780909</td>\n      <td>1.140966</td>\n      <td>-0.840691</td>\n      <td>-0.783780</td>\n      <td>-0.537614</td>\n      <td>-0.833251</td>\n      <td>-1.042521</td>\n      <td>-0.366301</td>\n      <td>0.117406</td>\n      <td>0.662145</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16507</th>\n      <td>0.722267</td>\n      <td>-0.673331</td>\n      <td>1.379547</td>\n      <td>-0.632123</td>\n      <td>-0.725361</td>\n      <td>-0.759010</td>\n      <td>-0.764049</td>\n      <td>0.554158</td>\n      <td>0.234352</td>\n      <td>-0.031755</td>\n      <td>-0.428853</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16508</th>\n      <td>1.007011</td>\n      <td>-0.823004</td>\n      <td>0.902385</td>\n      <td>-0.667196</td>\n      <td>-0.584183</td>\n      <td>-0.329664</td>\n      <td>-0.636291</td>\n      <td>-0.948815</td>\n      <td>-0.308114</td>\n      <td>0.084689</td>\n      <td>0.491503</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16509</th>\n      <td>1.586489</td>\n      <td>-0.724781</td>\n      <td>-1.562952</td>\n      <td>1.043901</td>\n      <td>0.822735</td>\n      <td>0.607904</td>\n      <td>0.713156</td>\n      <td>-0.316705</td>\n      <td>0.346934</td>\n      <td>-0.030554</td>\n      <td>-0.521776</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16510</th>\n      <td>0.782213</td>\n      <td>-0.851068</td>\n      <td>0.186642</td>\n      <td>-0.309919</td>\n      <td>-0.374849</td>\n      <td>-0.057178</td>\n      <td>-0.375451</td>\n      <td>0.098121</td>\n      <td>0.024995</td>\n      <td>0.061509</td>\n      <td>-0.303407</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16511</th>\n      <td>-1.435791</td>\n      <td>0.996459</td>\n      <td>1.856709</td>\n      <td>0.220853</td>\n      <td>0.360253</td>\n      <td>-0.135159</td>\n      <td>0.377791</td>\n      <td>-0.157799</td>\n      <td>-0.228529</td>\n      <td>-0.095863</td>\n      <td>0.101806</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16512 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "attributes\n",
    "def stripCols(old_set):\n",
    "    newTrainsetdf = pd.DataFrame(old_set, columns=attributes)\n",
    "    return newTrainsetdf.drop(columns=[\"ISLAND\",\"NEAR OCEAN\",\"NEAR BAY\",\"<1H OCEAN\"], axis=1)\n",
    "\n",
    "newTrainsetdfMKii = stripCols(trainingDataPrepped)\n",
    "\n",
    "newTrainsetdfMKii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "18792.68038879338"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 24
    }
   ],
   "source": [
    "improvedRF_Reg =RandomForestRegressor(max_features=6,n_estimators=50)\n",
    "improvedRF_Reg.fit(trainingDataPrepped,housing_labels)\n",
    "improvedRF_RegPredicts=improvedRF_Reg.predict(trainingDataPrepped)\n",
    "\n",
    "improvedRF_MSE=mean_squared_error(housing_labels,improvedRF_RegPredicts)\n",
    "improvedRF_RMSE=np.sqrt(improvedRF_MSE)\n",
    "improvedRF_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features=6, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=50,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 28
    }
   ],
   "source": [
    "gs2 = grid_search(newTrainsetdfMKii,housing_labels,nEstimators=[30, 40,50,60], maxFeatures=[ 4, 6, 8,10,12])\n",
    "gs2.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}